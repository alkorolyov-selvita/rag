{
 "cells": [
  {
   "cell_type": "code",
   "id": "4aae471b846bc8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:17:19.527949Z",
     "start_time": "2025-10-25T15:17:14.656204Z"
    }
   },
   "source": [
    "from rag.config import Settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import CrossEncoder\n",
    "from FlagEmbedding import FlagReranker\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rag.embeddings import LocalEmbedder\n",
    "from rag.utils import get_metrics, embed_dataset\n",
    "\n",
    "# Load datasets\n",
    "doc_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\")['passages']\n",
    "query_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\")['test']"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ergot/projects/rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:17:19.546243Z",
     "start_time": "2025-10-25T15:17:19.530393Z"
    }
   },
   "cell_type": "code",
   "source": "doc_ds = doc_ds.filter(lambda row: row['passage'] != 'nan')",
   "id": "622cd68dd8b7a32d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1e0672332a81af2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:17:19.655253Z",
     "start_time": "2025-10-25T15:17:19.572735Z"
    }
   },
   "source": [
    "# Precompute query information\n",
    "queries = np.array(query_ds['question'])\n",
    "qrels = [np.array(eval(gold)) for gold in query_ds['relevant_passage_ids']]\n",
    "qrels_counts = [len(s) for s in qrels]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e72228b3a496c365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:17:19.671443Z",
     "start_time": "2025-10-25T15:17:19.656820Z"
    }
   },
   "source": [
    "# Define chunking function\n",
    "def chunk_documents(dataset, chunker, text_col='passage', id_col='id'):\n",
    "    chunked_docs = []\n",
    "    pbar = tqdm(total=len(dataset), desc='Chunking')\n",
    "    for doc in dataset:\n",
    "        text = doc[text_col]\n",
    "        parent_id = doc[id_col]\n",
    "        chunks = chunker.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                'passage': chunk,\n",
    "                'parent_id': parent_id,\n",
    "                'chunk_id': i,\n",
    "            })\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return Dataset.from_list(chunked_docs)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "reranking_function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:17:19.711044Z",
     "start_time": "2025-10-25T15:17:19.698698Z"
    }
   },
   "source": "# Define reranking function (optimized for GPU)\ndef rerank_results(queries, retrieved_passages, retrieved_ids, reranker, batch_size=256):\n    \"\"\"\n    Rerank retrieved passages using a reranker model (optimized for GPU).\n    \n    Args:\n        queries: List of query strings\n        retrieved_passages: 2D array of retrieved passage texts [n_queries, k]\n        retrieved_ids: 2D array of retrieved passage IDs [n_queries, k]\n        reranker: Reranker model (CrossEncoder or FlagReranker)\n        batch_size: Batch size for reranking (larger is better for GPU utilization)\n    \n    Returns:\n        reranked_ids: 2D array of reranked passage IDs [n_queries, k]\n    \"\"\"\n    n_queries = len(queries)\n    k = retrieved_passages.shape[1]\n    \n    # Flatten all query-passage pairs to maximize GPU utilization\n    all_pairs = []\n    for i in range(n_queries):\n        query = queries[i]\n        passages = retrieved_passages[i]\n        pairs = [[query, passage] for passage in passages]\n        all_pairs.extend(pairs)\n    \n    # Score all pairs in one batch (much faster!)\n    print(f\"    Scoring {len(all_pairs):,} pairs with batch_size={batch_size}...\")\n    \n    # Use appropriate scoring method based on reranker type\n    if isinstance(reranker, FlagReranker):\n        # FlagReranker.compute_score expects list of [query, doc] pairs\n        all_scores = reranker.compute_score(all_pairs, batch_size=batch_size, normalize=True)\n        all_scores = np.array(all_scores)\n    else:\n        # CrossEncoder from sentence-transformers\n        all_scores = reranker.predict(all_pairs, batch_size=batch_size, show_progress_bar=True)\n    \n    # Reshape scores back to [n_queries, k]\n    scores_2d = all_scores.reshape(n_queries, k)\n    \n    # Sort by scores for each query\n    reranked_ids = np.zeros_like(retrieved_ids)\n    for i in range(n_queries):\n        sorted_indices = np.argsort(scores_2d[i])[::-1]\n        reranked_ids[i] = retrieved_ids[i][sorted_indices]\n    \n    return reranked_ids\n\n\ndef deduplicate_retrieved_ids(retrieved_ids, max_k=100):\n    \"\"\"\n    Deduplicate document IDs while preserving rank order.\n    \n    Args:\n        retrieved_ids: 2D array of retrieved IDs [n_queries, k] (may contain duplicates)\n        max_k: Maximum number of unique IDs to keep per query\n    \n    Returns:\n        deduplicated_ids: 2D array [n_queries, max_k] with duplicates removed, \n                         padded with zeros if fewer than max_k unique IDs\n    \"\"\"\n    n_queries = retrieved_ids.shape[0]\n    deduplicated = np.zeros((n_queries, max_k), dtype=retrieved_ids.dtype)\n    \n    for i in range(n_queries):\n        seen = set()\n        unique_ids = []\n        for doc_id in retrieved_ids[i]:\n            if doc_id not in seen and doc_id != 0:  # Skip padding zeros\n                seen.add(doc_id)\n                unique_ids.append(doc_id)\n                if len(unique_ids) >= max_k:\n                    break\n        \n        # Fill deduplicated array (remaining positions stay 0)\n        deduplicated[i, :len(unique_ids)] = unique_ids\n    \n    return deduplicated",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.682984Z",
     "start_time": "2025-10-25T15:17:19.741874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedder models to test\n",
    "embedder_models = [\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"BAAI/bge-small-en-v1.5\",\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "]\n",
    "\n",
    "# Reranker models to test\n",
    "reranker_models = [\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
    "    # \"BAAI/bge-reranker-base\",\n",
    "    # \"BAAI/bge-reranker-large\",\n",
    "]\n",
    "\n",
    "# Chunking parameters\n",
    "# chunk_size = 256\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Initial retrieval depth\n",
    "initial_k = 100\n",
    "\n",
    "for emb_idx, embedder_name in enumerate(embedder_models):\n",
    "    print(\"=\" * 20, f\"[{emb_idx + 1}/{len(embedder_models)}] Embedder: {embedder_name}\", \"=\" * 20)\n",
    "\n",
    "    for chunk_size in [128, 256]:\n",
    "        try:\n",
    "            # Create embedder\n",
    "            embedder = LocalEmbedder(embedder_name, device=\"cuda\")\n",
    "\n",
    "            # Create tokenizer-aware chunker\n",
    "            tokenizer = embedder.model.tokenizer\n",
    "            def count_tokens(text):\n",
    "                return len(tokenizer.encode(text))\n",
    "\n",
    "            chunker = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=count_tokens,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "            )\n",
    "\n",
    "            # Chunk documents\n",
    "            chunked_ds = chunk_documents(doc_ds, chunker)\n",
    "            print(f\"Created {len(chunked_ds)} chunks from {len(doc_ds)} documents\")\n",
    "\n",
    "            # Embed chunked documents and queries\n",
    "            embed_start = time()\n",
    "            chunked_ds = embed_dataset(chunked_ds, embedder, column='passage')\n",
    "            query_ds_embedded = embed_dataset(query_ds, embedder, column='question')\n",
    "            embed_time = time() - embed_start\n",
    "\n",
    "            # Create mapping from chunk index to parent document ID\n",
    "            index_to_parent_id = np.array(chunked_ds['parent_id'])\n",
    "            chunk_passages = np.array(chunked_ds['passage'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to embed with {embedder_name}: {e}\")\n",
    "            if 'embedder' in locals():\n",
    "                del embedder\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        # Test with both FAISS metrics\n",
    "        faiss_metric = 'IP'\n",
    "\n",
    "\n",
    "        # Add FAISS index\n",
    "        chunked_ds.add_faiss_index(\n",
    "            column='embedding',\n",
    "            string_factory='Flat',\n",
    "            metric_type=faiss.METRIC_L2 if faiss_metric == 'L2' else faiss.METRIC_INNER_PRODUCT,\n",
    "            batch_size=128,\n",
    "        )\n",
    "\n",
    "        # Retrieve top-k candidates\n",
    "        res = chunked_ds.get_index('embedding').search_batch(\n",
    "            np.array(query_ds_embedded['embedding']), k=initial_k\n",
    "        )\n",
    "\n",
    "        # Map chunk indices to parent document IDs\n",
    "        retrieved_chunk_ids = res.total_indices\n",
    "        retrieved_parent_ids = index_to_parent_id[retrieved_chunk_ids]\n",
    "        retrieved_passages = chunk_passages[retrieved_chunk_ids]\n",
    "\n",
    "        # Test each reranker\n",
    "        for reranker_idx, reranker_name in enumerate(reranker_models):\n",
    "            print(f\"    [{reranker_idx + 1}/{len(reranker_models)}] Reranker: {reranker_name}\")\n",
    "\n",
    "            try:\n",
    "                # Load reranker with correct library\n",
    "                # BAAI rerankers use FlagEmbedding, others use sentence-transformers CrossEncoder\n",
    "                if \"BAAI\" in reranker_name or \"bge-reranker\" in reranker_name:\n",
    "                    reranker = FlagReranker(reranker_name, use_fp16=True)\n",
    "                else:\n",
    "                    reranker = CrossEncoder(reranker_name, device=\"cuda\")\n",
    "\n",
    "                # Rerank results\n",
    "                rerank_start = time()\n",
    "                reranked_parent_ids = rerank_results(\n",
    "                    queries, retrieved_passages, retrieved_parent_ids, reranker\n",
    "                )\n",
    "\n",
    "                # DEDUPLICATE: Remove duplicate doc IDs while preserving rank order\n",
    "                # This is critical for accurate IR metrics\n",
    "                reranked_parent_ids_dedup = deduplicate_retrieved_ids(reranked_parent_ids, max_k=initial_k)\n",
    "                print(f\"    Deduplicated: {reranked_parent_ids.shape} -> unique docs per query\")\n",
    "\n",
    "                rerank_time = time() - rerank_start\n",
    "\n",
    "                total_time = embed_time + rerank_time\n",
    "\n",
    "                # Compute metrics for different k values (using deduplicated results)\n",
    "                metrics = {}\n",
    "                for k in [1, 3, 5, 10]:\n",
    "                    reranked_top_k = reranked_parent_ids_dedup[:, :k]\n",
    "                    metrics = {\n",
    "                        **metrics,\n",
    "                        **get_metrics(reranked_top_k, query_ds, k),\n",
    "                    }\n",
    "\n",
    "                # Save results\n",
    "                res_dict = {\n",
    "                    'model': embedder_name,\n",
    "                    'faiss_metric': faiss_metric,\n",
    "                    'chunked': True,\n",
    "                    'chunk_size': chunk_size,\n",
    "                    'chunk_overlap': chunk_overlap,\n",
    "                    'rerank_model': reranker_name,\n",
    "                    **{k: round(v, 3) for k, v in metrics.items()},\n",
    "                    \"elapsed_time\": round(total_time, 1),\n",
    "                }\n",
    "\n",
    "                res_df = pd.DataFrame([res_dict])\n",
    "                csv_path = \"results.csv\"\n",
    "                append = os.path.exists(csv_path) and os.path.getsize(csv_path) > 0\n",
    "                res_df.to_csv(csv_path, mode='a', header=not append, index=False)\n",
    "\n",
    "                # Print summary\n",
    "                print(f\"      P@10: {metrics['P@10']:.3f}, R@10: {metrics['R@10']:.3f}, \"\n",
    "                      f\"MRR@10: {metrics['MRR@10']:.3f}, nDCG@10: {metrics['nDCG@10']:.3f}, \"\n",
    "                      f\"Time: {total_time:.1f}s\")\n",
    "\n",
    "                # Clean up reranker\n",
    "                del reranker\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      Failed to rerank with {reranker_name}: {e}\")\n",
    "                if 'reranker' in locals():\n",
    "                    del reranker\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        # Remove FAISS index for next metric\n",
    "        chunked_ds.drop_index('embedding')\n",
    "\n",
    "        # Clean up embedder\n",
    "        del embedder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n",
    "\n",
    "print(\"\\nReranking comparison complete! Results saved to results.csv\")"
   ],
   "id": "ef3e0181aed0bd0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [1/5] Embedder: all-MiniLM-L6-v2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:42<00:00, 657.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 146055 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 146055/146055 [02:27<00:00, 991.61 examples/s] \n",
      "Map: 100%|██████████| 4719/4719 [00:04<00:00, 948.96 examples/s] \n",
      "100%|██████████| 1142/1142 [00:00<00:00, 4200.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [01:12<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.361, R@10: 0.475, MRR@10: 0.771, nDCG@10: 0.600, Time: 249.1s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:19<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.361, R@10: 0.476, MRR@10: 0.773, nDCG@10: 0.602, Time: 315.1s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:42<00:00, 659.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 64042 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 64042/64042 [01:40<00:00, 636.12 examples/s] \n",
      "100%|██████████| 501/501 [00:00<00:00, 4901.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:12<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.355, R@10: 0.467, MRR@10: 0.764, nDCG@10: 0.593, Time: 257.3s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [04:19<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.356, R@10: 0.467, MRR@10: 0.767, nDCG@10: 0.594, Time: 384.8s\n",
      "\n",
      "==================== [2/5] Embedder: all-MiniLM-L12-v2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:44<00:00, 633.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 146055 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 146055/146055 [04:11<00:00, 581.40 examples/s] \n",
      "Map: 100%|██████████| 4719/4719 [00:06<00:00, 759.26 examples/s] \n",
      "100%|██████████| 1142/1142 [00:00<00:00, 4550.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [01:12<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.351, R@10: 0.463, MRR@10: 0.764, nDCG@10: 0.588, Time: 355.7s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:18<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.352, R@10: 0.464, MRR@10: 0.767, nDCG@10: 0.590, Time: 422.3s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:43<00:00, 650.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 64042 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 64042/64042 [01:52<00:00, 570.37 examples/s] \n",
      "100%|██████████| 501/501 [00:00<00:00, 4719.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:13<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.348, R@10: 0.456, MRR@10: 0.760, nDCG@10: 0.583, Time: 270.7s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [04:18<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.348, R@10: 0.457, MRR@10: 0.765, nDCG@10: 0.585, Time: 396.5s\n",
      "\n",
      "==================== [3/5] Embedder: BAAI/bge-small-en-v1.5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:46<00:00, 597.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 146055 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 146055/146055 [04:12<00:00, 578.84 examples/s] \n",
      "Map: 100%|██████████| 4719/4719 [00:05<00:00, 810.77 examples/s] \n",
      "100%|██████████| 1142/1142 [00:00<00:00, 4360.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [01:13<00:00, 25.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.377, R@10: 0.501, MRR@10: 0.784, nDCG@10: 0.623, Time: 357.4s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:19<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.377, R@10: 0.501, MRR@10: 0.787, nDCG@10: 0.625, Time: 423.2s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:43<00:00, 643.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 64042 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 64042/64042 [02:10<00:00, 490.34 examples/s]\n",
      "100%|██████████| 501/501 [00:00<00:00, 4534.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:13<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.377, R@10: 0.499, MRR@10: 0.784, nDCG@10: 0.624, Time: 290.4s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [04:21<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.377, R@10: 0.500, MRR@10: 0.789, nDCG@10: 0.625, Time: 418.4s\n",
      "\n",
      "==================== [4/5] Embedder: BAAI/bge-base-en-v1.5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:48<00:00, 573.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 146055 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 146055/146055 [05:23<00:00, 450.97 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:08<00:00, 568.10 examples/s]\n",
      "100%|██████████| 1142/1142 [00:00<00:00, 2753.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [01:13<00:00, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.383, R@10: 0.511, MRR@10: 0.789, nDCG@10: 0.632, Time: 432.7s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:17<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.384, R@10: 0.512, MRR@10: 0.791, nDCG@10: 0.634, Time: 496.1s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:42<00:00, 655.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 64042 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 64042/64042 [02:54<00:00, 367.12 examples/s]\n",
      "100%|██████████| 501/501 [00:00<00:00, 3315.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:15<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.381, R@10: 0.508, MRR@10: 0.789, nDCG@10: 0.631, Time: 337.5s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [04:23<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.382, R@10: 0.509, MRR@10: 0.793, nDCG@10: 0.632, Time: 465.6s\n",
      "\n",
      "==================== [5/5] Embedder: BAAI/bge-large-en-v1.5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:48<00:00, 578.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 146055 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 146055/146055 [10:40<00:00, 227.98 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:12<00:00, 378.79 examples/s]\n",
      "100%|██████████| 1142/1142 [00:00<00:00, 2471.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [01:12<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.383, R@10: 0.509, MRR@10: 0.787, nDCG@10: 0.630, Time: 754.3s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:21<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.383, R@10: 0.509, MRR@10: 0.788, nDCG@10: 0.632, Time: 822.1s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 28001/28001 [00:43<00:00, 644.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 64042 chunks from 28001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 64042/64042 [06:43<00:00, 158.74 examples/s]\n",
      "100%|██████████| 501/501 [00:00<00:00, 2850.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/2] Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:16<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.383, R@10: 0.509, MRR@10: 0.787, nDCG@10: 0.631, Time: 569.2s\n",
      "    [2/2] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [04:22<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Deduplicated: (4719, 100) -> unique docs per query\n",
      "      P@10: 0.383, R@10: 0.509, MRR@10: 0.791, nDCG@10: 0.632, Time: 695.0s\n",
      "\n",
      "\n",
      "Reranking comparison complete! Results saved to results.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.732549Z",
     "start_time": "2025-10-25T17:06:46.731235Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ccadf8df8cfe92dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.775291Z",
     "start_time": "2025-10-25T17:06:46.773931Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f876edf89a2fc45a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.817012Z",
     "start_time": "2025-10-25T17:06:46.815710Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "51ac7d4f7f883b8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.859050Z",
     "start_time": "2025-10-25T17:06:46.857740Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ad4337acc23c9ac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.900996Z",
     "start_time": "2025-10-25T17:06:46.899686Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9504a9410213317c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.943444Z",
     "start_time": "2025-10-25T17:06:46.941798Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6763a35341ef337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:46.986220Z",
     "start_time": "2025-10-25T17:06:46.984839Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a321a3b1d7f136e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:47.028003Z",
     "start_time": "2025-10-25T17:06:47.026703Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "213428be58ddbf2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:47.069946Z",
     "start_time": "2025-10-25T17:06:47.068649Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9f4912bfb325dd0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:06:47.112311Z",
     "start_time": "2025-10-25T17:06:47.110809Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6e2dad911d5c85bc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
