{
 "cells": [
  {
   "cell_type": "code",
   "id": "4aae471b846bc8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T05:11:08.451991Z",
     "start_time": "2025-10-24T05:11:06.450275Z"
    }
   },
   "source": [
    "from rag.config import Settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import CrossEncoder\n",
    "from FlagEmbedding import FlagReranker\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rag.embeddings import LocalEmbedder\n",
    "from rag.utils import get_metrics, embed_dataset\n",
    "\n",
    "# Load datasets\n",
    "doc_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\")['passages']\n",
    "query_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\")['test']"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "1e0672332a81af2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T05:11:08.559702Z",
     "start_time": "2025-10-24T05:11:08.455885Z"
    }
   },
   "source": [
    "# Precompute query information\n",
    "queries = np.array(query_ds['question'])\n",
    "qrels = [np.array(eval(gold)) for gold in query_ds['relevant_passage_ids']]\n",
    "qrels_counts = [len(s) for s in qrels]"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "e72228b3a496c365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T05:11:08.574525Z",
     "start_time": "2025-10-24T05:11:08.561647Z"
    }
   },
   "source": [
    "# Define chunking function\n",
    "def chunk_documents(dataset, chunker, text_col='passage', id_col='id'):\n",
    "    chunked_docs = []\n",
    "    pbar = tqdm(total=len(dataset), desc='Chunking')\n",
    "    for doc in dataset:\n",
    "        text = doc[text_col]\n",
    "        parent_id = doc[id_col]\n",
    "        chunks = chunker.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                'passage': chunk,\n",
    "                'parent_id': parent_id,\n",
    "                'chunk_id': i,\n",
    "            })\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return Dataset.from_list(chunked_docs)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "reranking_function",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T05:11:08.618358Z",
     "start_time": "2025-10-24T05:11:08.605049Z"
    }
   },
   "source": [
    "# Define reranking function (optimized for GPU)\n",
    "def rerank_results(queries, retrieved_passages, retrieved_ids, reranker, batch_size=256):\n",
    "    \"\"\"\n",
    "    Rerank retrieved passages using a reranker model (optimized for GPU).\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query strings\n",
    "        retrieved_passages: 2D array of retrieved passage texts [n_queries, k]\n",
    "        retrieved_ids: 2D array of retrieved passage IDs [n_queries, k]\n",
    "        reranker: Reranker model (CrossEncoder or FlagReranker)\n",
    "        batch_size: Batch size for reranking (larger is better for GPU utilization)\n",
    "    \n",
    "    Returns:\n",
    "        reranked_ids: 2D array of reranked passage IDs [n_queries, k]\n",
    "    \"\"\"\n",
    "    n_queries = len(queries)\n",
    "    k = retrieved_passages.shape[1]\n",
    "    \n",
    "    # Flatten all query-passage pairs to maximize GPU utilization\n",
    "    all_pairs = []\n",
    "    for i in range(n_queries):\n",
    "        query = queries[i]\n",
    "        passages = retrieved_passages[i]\n",
    "        pairs = [[query, passage] for passage in passages]\n",
    "        all_pairs.extend(pairs)\n",
    "    \n",
    "    # Score all pairs in one batch (much faster!)\n",
    "    print(f\"    Scoring {len(all_pairs):,} pairs with batch_size={batch_size}...\")\n",
    "    \n",
    "    # Use appropriate scoring method based on reranker type\n",
    "    if isinstance(reranker, FlagReranker):\n",
    "        # FlagReranker.compute_score expects list of [query, doc] pairs\n",
    "        all_scores = reranker.compute_score(all_pairs, batch_size=batch_size, normalize=True)\n",
    "        all_scores = np.array(all_scores)\n",
    "    else:\n",
    "        # CrossEncoder from sentence-transformers\n",
    "        all_scores = reranker.predict(all_pairs, batch_size=batch_size, show_progress_bar=True)\n",
    "    \n",
    "    # Reshape scores back to [n_queries, k]\n",
    "    scores_2d = all_scores.reshape(n_queries, k)\n",
    "    \n",
    "    # Sort by scores for each query\n",
    "    reranked_ids = np.zeros_like(retrieved_ids)\n",
    "    for i in range(n_queries):\n",
    "        sorted_indices = np.argsort(scores_2d[i])[::-1]\n",
    "        reranked_ids[i] = retrieved_ids[i][sorted_indices]\n",
    "    \n",
    "    return reranked_ids"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T05:16:01.124612Z",
     "start_time": "2025-10-24T05:11:08.649340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedder models to test\n",
    "embedder_models = [\n",
    "    # \"all-MiniLM-L6-v2\",\n",
    "    # \"all-MiniLM-L12-v2\",\n",
    "    \"BAAI/bge-small-en-v1.5\",\n",
    "    # \"BAAI/bge-base-en-v1.5\",\n",
    "    # \"BAAI/bge-large-en-v1.5\",\n",
    "]\n",
    "\n",
    "# Reranker models to test\n",
    "reranker_models = [\n",
    "    # \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
    "    # \"BAAI/bge-reranker-base\",\n",
    "    # \"BAAI/bge-reranker-large\",\n",
    "]\n",
    "\n",
    "# Chunking parameters\n",
    "chunk_size = 256\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Initial retrieval depth\n",
    "initial_k = 100\n",
    "\n",
    "for emb_idx, embedder_name in enumerate(embedder_models):\n",
    "    print(\"=\" * 20, f\"[{emb_idx + 1}/{len(embedder_models)}] Embedder: {embedder_name}\", \"=\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Create embedder\n",
    "        embedder = LocalEmbedder(embedder_name, device=\"cuda\")\n",
    "        \n",
    "        # Create tokenizer-aware chunker\n",
    "        tokenizer = embedder.model.tokenizer\n",
    "        def count_tokens(text):\n",
    "            return len(tokenizer.encode(text))\n",
    "        \n",
    "        chunker = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=count_tokens,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        )\n",
    "        \n",
    "        # Chunk documents\n",
    "        chunked_ds = chunk_documents(doc_ds, chunker)\n",
    "        print(f\"Created {len(chunked_ds)} chunks from {len(doc_ds)} documents\")\n",
    "        \n",
    "        # Embed chunked documents and queries\n",
    "        embed_start = time()\n",
    "        chunked_ds = embed_dataset(chunked_ds, embedder, column='passage')\n",
    "        query_ds_embedded = embed_dataset(query_ds, embedder, column='question')\n",
    "        embed_time = time() - embed_start\n",
    "        \n",
    "        # Create mapping from chunk index to parent document ID\n",
    "        index_to_parent_id = np.array(chunked_ds['parent_id'])\n",
    "        chunk_passages = np.array(chunked_ds['passage'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed with {embedder_name}: {e}\")\n",
    "        if 'embedder' in locals():\n",
    "            del embedder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    \n",
    "    # Test with both FAISS metrics\n",
    "    faiss_metric = 'IP'\n",
    "\n",
    "\n",
    "    # Add FAISS index\n",
    "    chunked_ds.add_faiss_index(\n",
    "        column='embedding',\n",
    "        string_factory='Flat',\n",
    "        metric_type=faiss.METRIC_L2 if faiss_metric == 'L2' else faiss.METRIC_INNER_PRODUCT,\n",
    "        batch_size=128,\n",
    "    )\n",
    "\n",
    "    # Retrieve top-k candidates\n",
    "    res = chunked_ds.get_index('embedding').search_batch(\n",
    "        np.array(query_ds_embedded['embedding']), k=initial_k\n",
    "    )\n",
    "\n",
    "    # Map chunk indices to parent document IDs\n",
    "    retrieved_chunk_ids = res.total_indices\n",
    "    retrieved_parent_ids = index_to_parent_id[retrieved_chunk_ids]\n",
    "    retrieved_passages = chunk_passages[retrieved_chunk_ids]\n",
    "\n",
    "    # Test each reranker\n",
    "    for reranker_idx, reranker_name in enumerate(reranker_models):\n",
    "        print(f\"    [{reranker_idx + 1}/{len(reranker_models)}] Reranker: {reranker_name}\")\n",
    "\n",
    "        try:\n",
    "            # Load reranker with correct library\n",
    "            # BAAI rerankers use FlagEmbedding, others use sentence-transformers CrossEncoder\n",
    "            if \"BAAI\" in reranker_name or \"bge-reranker\" in reranker_name:\n",
    "                reranker = FlagReranker(reranker_name, use_fp16=True)\n",
    "            else:\n",
    "                reranker = CrossEncoder(reranker_name, device=\"cuda\")\n",
    "\n",
    "            # Rerank results\n",
    "            rerank_start = time()\n",
    "            reranked_parent_ids = rerank_results(\n",
    "                queries, retrieved_passages, retrieved_parent_ids, reranker\n",
    "            )\n",
    "            rerank_time = time() - rerank_start\n",
    "\n",
    "            total_time = embed_time + rerank_time\n",
    "\n",
    "            # Compute metrics for different k values\n",
    "            metrics = {}\n",
    "            for k in [1, 3, 5, 10]:\n",
    "                reranked_top_k = reranked_parent_ids[:, :k]\n",
    "                metrics = {\n",
    "                    **metrics,\n",
    "                    **get_metrics(reranked_top_k, query_ds, k),\n",
    "                }\n",
    "\n",
    "            # Save results\n",
    "            res_dict = {\n",
    "                'model': embedder_name,\n",
    "                'faiss_metric': faiss_metric,\n",
    "                'chunked': True,\n",
    "                'chunk_size': chunk_size,\n",
    "                'chunk_overlap': chunk_overlap,\n",
    "                'rerank_model': reranker_name,\n",
    "                **{k: round(v, 3) for k, v in metrics.items()},\n",
    "                \"elapsed_time\": round(total_time, 1),\n",
    "            }\n",
    "\n",
    "            res_df = pd.DataFrame([res_dict])\n",
    "            csv_path = \"results.csv\"\n",
    "            append = os.path.exists(csv_path) and os.path.getsize(csv_path) > 0\n",
    "            res_df.to_csv(csv_path, mode='a', header=not append, index=False)\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"      P@10: {metrics['P@10']:.3f}, R@10: {metrics['R@10']:.3f}, \"\n",
    "                  f\"MRR@10: {metrics['MRR@10']:.3f}, nDCG@10: {metrics['nDCG@10']:.3f}, \"\n",
    "                  f\"Time: {total_time:.1f}s\")\n",
    "\n",
    "            # Clean up reranker\n",
    "            del reranker\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      Failed to rerank with {reranker_name}: {e}\")\n",
    "            if 'reranker' in locals():\n",
    "                del reranker\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Remove FAISS index for next metric\n",
    "    chunked_ds.drop_index('embedding')\n",
    "    \n",
    "    # Clean up embedder\n",
    "    del embedder\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print()\n",
    "\n",
    "print(\"\\nReranking comparison complete! Results saved to results.csv\")"
   ],
   "id": "ef3e0181aed0bd0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [1/1] Embedder: BAAI/bge-small-en-v1.5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:38<00:00, 1048.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [01:03<00:00, 1202.47 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:02<00:00, 2134.04 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 4680.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [1/1] Reranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    Scoring 471,900 pairs with batch_size=256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1844/1844 [02:37<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      P@10: 0.443, R@10: 0.681, MRR@10: 0.787, nDCG@10: 0.734, Time: 244.6s\n",
      "\n",
      "\n",
      "Reranking comparison complete! Results saved to results.csv\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ccadf8df8cfe92dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f876edf89a2fc45a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "51ac7d4f7f883b8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad4337acc23c9ac7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9504a9410213317c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6763a35341ef337"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a321a3b1d7f136e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "213428be58ddbf2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f4912bfb325dd0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T05:16:01.216475Z",
     "start_time": "2025-10-24T05:16:01.214666Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6e2dad911d5c85bc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
