{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T07:38:43.342535Z",
     "start_time": "2025-10-23T07:38:37.970944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rag.embeddings import LocalEmbedder\n",
    "from rag.utils import get_metrics, embed_dataset\n",
    "\n",
    "# Load datasets\n",
    "doc_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\")['passages']\n",
    "query_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"question-answer-passages\")['test']"
   ],
   "id": "4aae471b846bc8b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ergot/projects/rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T07:38:43.443467Z",
     "start_time": "2025-10-23T07:38:43.345482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Precompute query information\n",
    "queries = np.array(query_ds['question'])\n",
    "qrels = [np.array(eval(gold)) for gold in query_ds['relevant_passage_ids']]\n",
    "qrels_counts = [len(s) for s in qrels]"
   ],
   "id": "1e0672332a81af2f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T07:38:43.457863Z",
     "start_time": "2025-10-23T07:38:43.445878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define chunking function\n",
    "def chunk_documents(dataset, chunker, text_col='passage', id_col='id'):\n",
    "    chunked_docs = []\n",
    "    pbar = tqdm(total=len(dataset), desc='Chunking')\n",
    "    for doc in dataset:\n",
    "        text = doc[text_col]\n",
    "        parent_id = doc[id_col]\n",
    "        chunks = chunker.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_docs.append({\n",
    "                'passage': chunk,\n",
    "                'parent_id': parent_id,\n",
    "                'chunk_id': i,\n",
    "            })\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return Dataset.from_list(chunked_docs)\n"
   ],
   "id": "e72228b3a496c365",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T08:28:39.874135Z",
     "start_time": "2025-10-23T07:38:43.488751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedder models to compare\n",
    "embedder_models = [\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    \"BAAI/bge-small-en-v1.5\",\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"Alibaba-NLP/gte-multilingual-base\",\n",
    "    \"Snowflake/snowflake-arctic-embed-l-v2.0\",\n",
    "    \"jinaai/jina-embeddings-v3\",\n",
    "    \"intfloat/e5-base-v2\",\n",
    "    \"BAAI/bge-m3\",\n",
    "    \"Lajavaness/bilingual-embedding-base\",\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "]\n",
    "\n",
    "# Chunking parameters\n",
    "chunk_size = 256\n",
    "chunk_overlap = 50\n",
    "\n",
    "for i, model_name in enumerate(embedder_models):\n",
    "    print(\"=\" * 20, f\"[{i + 1}/{len(embedder_models)}]\", \"=\" * 20)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create embedder\n",
    "        embedder = LocalEmbedder(model_name, device=\"cuda\")\n",
    "        \n",
    "        # Create tokenizer-aware chunker\n",
    "        tokenizer = embedder.model.tokenizer\n",
    "        def count_tokens(text):\n",
    "            return len(tokenizer.encode(text))\n",
    "        \n",
    "        chunker = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=count_tokens,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        )\n",
    "        \n",
    "        # Chunk documents\n",
    "        chunked_ds = chunk_documents(doc_ds, chunker)\n",
    "        print(f\"Created {len(chunked_ds)} chunks from {len(doc_ds)} documents\")\n",
    "        \n",
    "        start_time = time()\n",
    "        # Embed chunked documents and queries\n",
    "        chunked_ds = embed_dataset(chunked_ds, embedder, column='passage')\n",
    "        query_ds = embed_dataset(query_ds, embedder, column='question')\n",
    "        elapsed_time = time() - start_time\n",
    "        \n",
    "        # Create mapping from chunk index to parent document ID\n",
    "        index_to_parent_id = np.array(chunked_ds['parent_id'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed {model_name}: {e}\")\n",
    "        if 'embedder' in locals():\n",
    "            del embedder\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    \n",
    "    # Test with both FAISS metrics\n",
    "    for faiss_metric in [\"IP\", \"L2\"]:\n",
    "        # Add FAISS index\n",
    "        chunked_ds.add_faiss_index(\n",
    "            column='embedding',\n",
    "            string_factory='Flat',\n",
    "            metric_type=faiss.METRIC_L2 if faiss_metric == 'L2' else faiss.METRIC_INNER_PRODUCT,\n",
    "            batch_size=128,\n",
    "        )\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Compute metrics for different k values\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            res = chunked_ds.get_index('embedding').search_batch(\n",
    "                np.array(query_ds['embedding']), k=k\n",
    "            )\n",
    "            # Map chunk indices to parent document IDs\n",
    "            retrieved_ids = index_to_parent_id[res.total_indices]\n",
    "            \n",
    "            metrics = {\n",
    "                **metrics,\n",
    "                **get_metrics(retrieved_ids, query_ds, k),\n",
    "            }\n",
    "        \n",
    "        # Save results\n",
    "        res_dict = {\n",
    "            'model': model_name,\n",
    "            'faiss_metric': faiss_metric,\n",
    "            'chunked': True,\n",
    "            'chunk_size': None,\n",
    "            'chunk_overlap': None,\n",
    "            'rerank_model': None,\n",
    "            **{k: round(v, 3) for k, v in metrics.items()},\n",
    "            \"elapsed_time\": round(elapsed_time, 1),\n",
    "        }\n",
    "        \n",
    "        res_df = pd.DataFrame([res_dict])\n",
    "        csv_path = \"results.csv\"\n",
    "        append = os.path.exists(csv_path) and os.path.getsize(csv_path) > 0\n",
    "        res_df.to_csv(csv_path, mode='a', header=not append, index=False)\n",
    "        \n",
    "        # Remove FAISS index for next metric\n",
    "        chunked_ds.drop_index('embedding')\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"P@10    {metrics['P@10']:.3f}\")\n",
    "    print(f\"R@10    {metrics['R@10']:.3f}\")\n",
    "    print(f\"MRR@10  {metrics['MRR@10']:.3f}\")\n",
    "    print(f\"nDCG@10 {metrics['nDCG@10']:.3f}\")\n",
    "    print(f\"Time: {elapsed_time:.1f}s\")\n",
    "    print()\n",
    "    \n",
    "# Clean up\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Comparison complete! Results saved to results.csv\")"
   ],
   "id": "e03354fcb3be0f46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [1/14] ====================\n",
      "Model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:36<00:00, 1094.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [00:40<00:00, 1893.54 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:01<00:00, 3093.77 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 4391.82it/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 4627.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.356\n",
      "R@10    0.538\n",
      "MRR@10  0.651\n",
      "nDCG@10 0.573\n",
      "Time: 56.6s\n",
      "\n",
      "==================== [2/14] ====================\n",
      "Model: all-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:37<00:00, 1075.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [00:51<00:00, 1495.24 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:02<00:00, 2068.20 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 4358.89it/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 4408.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.331\n",
      "R@10    0.496\n",
      "MRR@10  0.626\n",
      "nDCG@10 0.535\n",
      "Time: 68.9s\n",
      "\n",
      "==================== [3/14] ====================\n",
      "Model: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:35<00:00, 1139.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [01:47<00:00, 710.80 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:03<00:00, 1458.66 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 2645.10it/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 2828.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.336\n",
      "R@10    0.508\n",
      "MRR@10  0.609\n",
      "nDCG@10 0.538\n",
      "Time: 127.0s\n",
      "\n",
      "==================== [4/14] ====================\n",
      "Model: nomic-ai/nomic-embed-text-v1.5\n",
      "Failed to embed nomic-ai/nomic-embed-text-v1.5: nomic-ai/nomic-bert-2048 You can inspect the repository content at https://hf.co/nomic-ai/nomic-embed-text-v1.5.\n",
      "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
      "==================== [5/14] ====================\n",
      "Model: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:39<00:00, 1013.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [01:03<00:00, 1195.04 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:02<00:00, 2089.07 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 4142.60it/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 4265.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.419\n",
      "R@10    0.652\n",
      "MRR@10  0.750\n",
      "nDCG@10 0.694\n",
      "Time: 81.7s\n",
      "\n",
      "==================== [6/14] ====================\n",
      "Model: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:37<00:00, 1082.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [01:39<00:00, 765.44 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:03<00:00, 1557.03 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 2768.41it/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 2839.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.437\n",
      "R@10    0.687\n",
      "MRR@10  0.757\n",
      "nDCG@10 0.721\n",
      "Time: 119.3s\n",
      "\n",
      "==================== [7/14] ====================\n",
      "Model: BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:37<00:00, 1075.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [04:04<00:00, 311.76 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:06<00:00, 747.42 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 2398.06it/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 2454.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.442\n",
      "R@10    0.699\n",
      "MRR@10  0.760\n",
      "nDCG@10 0.731\n",
      "Time: 267.8s\n",
      "\n",
      "==================== [8/14] ====================\n",
      "Model: Alibaba-NLP/gte-multilingual-base\n",
      "Failed to embed Alibaba-NLP/gte-multilingual-base: Alibaba-NLP/new-impl You can inspect the repository content at https://hf.co/Alibaba-NLP/gte-multilingual-base.\n",
      "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
      "==================== [9/14] ====================\n",
      "Model: Snowflake/snowflake-arctic-embed-l-v2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:38<00:00, 1033.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 82827 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 82827/82827 [04:29<00:00, 307.75 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:06<00:00, 705.74 examples/s]\n",
      "100%|██████████| 648/648 [00:00<00:00, 2440.32it/s]\n",
      "100%|██████████| 648/648 [00:00<00:00, 2466.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.402\n",
      "R@10    0.644\n",
      "MRR@10  0.711\n",
      "nDCG@10 0.667\n",
      "Time: 293.9s\n",
      "\n",
      "==================== [10/14] ====================\n",
      "Model: jinaai/jina-embeddings-v3\n",
      "Failed to embed jinaai/jina-embeddings-v3: No module named 'custom_st'\n",
      "==================== [11/14] ====================\n",
      "Model: intfloat/e5-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:45<00:00, 883.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 76262 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 76262/76262 [01:40<00:00, 757.66 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:03<00:00, 1533.29 examples/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 3084.96it/s]\n",
      "100%|██████████| 596/596 [00:00<00:00, 3764.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.422\n",
      "R@10    0.662\n",
      "MRR@10  0.752\n",
      "nDCG@10 0.700\n",
      "Time: 120.5s\n",
      "\n",
      "==================== [12/14] ====================\n",
      "Model: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:38<00:00, 1055.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 82827 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 82827/82827 [04:24<00:00, 312.73 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:06<00:00, 693.38 examples/s]\n",
      "100%|██████████| 648/648 [00:00<00:00, 3180.72it/s]\n",
      "100%|██████████| 648/648 [00:00<00:00, 3337.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.415\n",
      "R@10    0.663\n",
      "MRR@10  0.745\n",
      "nDCG@10 0.694\n",
      "Time: 290.1s\n",
      "\n",
      "==================== [13/14] ====================\n",
      "Model: Lajavaness/bilingual-embedding-base\n",
      "Failed to embed Lajavaness/bilingual-embedding-base: dangvantuan/bilingual_impl You can inspect the repository content at https://hf.co/Lajavaness/bilingual-embedding-base.\n",
      "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
      "==================== [14/14] ====================\n",
      "Model: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|██████████| 40221/40221 [00:46<00:00, 871.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 75905 chunks from 40221 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75905/75905 [06:38<00:00, 190.68 examples/s]\n",
      "Map: 100%|██████████| 4719/4719 [00:10<00:00, 461.84 examples/s]\n",
      "100%|██████████| 594/594 [00:00<00:00, 1337.34it/s]\n",
      "100%|██████████| 594/594 [00:00<00:00, 2221.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10    0.405\n",
      "R@10    0.639\n",
      "MRR@10  0.715\n",
      "nDCG@10 0.670\n",
      "Time: 426.3s\n",
      "\n",
      "Comparison complete! Results saved to results.csv\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
